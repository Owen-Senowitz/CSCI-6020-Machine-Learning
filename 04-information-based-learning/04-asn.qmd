---
title: "Written Assignment 04"
author: "Your Name"
date: today
number-sections: true
number-depth: 3
format:
  html:
    toc: true
    toc-location: right
    number-sections: true
    number-depth: 3
    html-math-method: katex
    embed-resources: true
# bibliography: dasc-6000.bib 
# csl: ieee-with-url.csl
# linkcolor: red
# urlcolor: blue
# link-citations: yes
# header-includes:
#   - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Assignment Goal {.unnumbered}

The goal of this assignment is to demonstrate your understanding of how the concepts from information theory can be used to build machine learning models for predictive tasks. 



# Calculating Entropy

The figure below shows a set of eight Scrabble pieces.

![](./scrabble.png)



1. What is the entropy (in bits) of the letters in this set?
    
    Your answer goes here.
    
1. What would be the reduction in entropy (i.e., the information gain) in bits if we split these letters into two sets, one containing the vowels and the other containing the consonants?
    
    Your answer goes here.
    
1. What is the maximum possible entropy (in bits) for a set of eight Scrabble pieces?
    
    Your answer goes here.
    
1. In general, which is preferable when you are playing Scrabble: a set of letters with high entropy, or a set of letters with low entropy?
    
    Your answer goes here.
    
# Decision Tree Construction

A convicted criminal who reoffends after release is known as a recidivist. The Table below lists a dataset that describes prisoners released on parole, and whether they reoffended within two years of release.

```{r}
#| echo: false
c1 <- c(1,2,3,4,5,6)
c2 <- c("false", "false", "false", "true", "true", "true")
c3 <- c("true", "false", "true", "false", "false", "false")
c4 <- c("false", "false", "false", "false", "true", "false")
c5 <- c("true", "false", "true", "false", "true", "false")
df1 <- data.frame(c1, c2, c3, c4, c5)
# print(df1)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df1, caption = 'Prisoners on parole.', col.names = c('ID', 'Good Behavior', 'Age Below 30', 'Drug Dependent', 'Recidivist'), align = "lrrrr")
```



This dataset lists six instances where prisoners were granted parole. Each of these instances are described in terms of three binary descriptive features (**Good Behavior**, **Age Below 30**, **Drug Dependent**) and a binary target feature, **Recidivist**. The **Good Behavior** feature has a value of **true** if the prisoner had not committed any infringements during incarceration, the **Age Below 30** has a value of **true** if the prisoner was under 30 years of age when granted parole, and the **Drug Dependent** feature is **true** if the prisoner had a drug addiction at the time of parole. The target feature, **Recidivist**, has a **true** value if the prisoner was arrested within two years of being released; otherwise it has a value of **false**.


1. Using this dataset, construct the decision tree that would be generated by the ID3 algorithm, using entropy-based information gain.
    
    Your answer goes here.
    
1. What prediction will the decision tree generated in part (a) of this question return for the following query?
    
    **Good Behavior** = false, **Age Below 30** = false, **Drug Dependent** = true
    
    Your answer goes here.
    
1. What prediction will the decision tree generated in part (a) of this question return for the following query?

    **Good Behavior** = true, **Age Below 30** = true, **Drug Dependent** = false
    
    Your answer goes here.
    
# Information Gain

The Table below lists a sample of data from a census.

```{r}
#| echo: false
c1 <- c(1,2,3,4,5,6,7,8)
c2 <- c(39, 50, 18, 28, 37, 24, 52, 40)
c3 <- c('bachelors', 'bachelors', 'high school', 'bachelors', 'high school', 'high school', 'high school', 'doctorate')
c4 <- c('never married', 'married', 'never married', 'married', 'married', 'never married', 'divorced', 'married')
c5 <- c('transport', 'professional', 'agriculture', 'professional', 'agriculture', 'armed forces', 'transport', 'professional')
c6 <- c('25K-50K', '25K-50K', 'below 25K', '25K-50K', '25K-50K', 'below 25K', '25K-50K', 'over 50K')
df2 <- data.frame(c1,c2,c3,c4,c5,c6)
# print(df2)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df2, caption = 'Census data.', col.names = c('ID', 'Age', 'Education', 'Marital Status', 'Occupation', 'Annual Income'), align = "lrrrrr")
```


There are four descriptive features and one target feature in this dataset:

- **Age**, a continuous feature listing the age of the individual.
- **Education**, a categorical feature listing the highest education award achieved by the individual (high school, bachelors, doctorate).
- **Marital Status** (never married, married, divorced).
- **Occupation** (transport = works in the transportation industry; professional = doctors, lawyers, etc.; agriculture = works in the agricultural industry; armed forces = is a member of the armed forces).
- **Annual Income**, the target feature with 3 levels (<25K, 25K-50K, >50K).


1. Calculate the **entropy** for this dataset.
    
    Your answer goes here.

1. Calculate the **Gini** index for this dataset.
    
    Your answer goes here.

1. When building a decision tree, the easiest way to handle a continuous feature is to define a threshold around which splits will be made. What would be the optimal threshold to split the continuous **Age** feature (use information gain based on entropy as the feature selection measure)?
    
    Your answer goes here.

1. Calculate information gain (based on entropy) for the **Education**, **Marital Status**, and **Occupation** features.
    
    Your answer goes here.

1. Calculate the information gain ratio (based on entropy) for **Education**, **Marital Status**, and **Occupation** features.
    
    Your answer goes here.
    
1. Calculate information gain using the Gini index for the **Education**, **Marital Status**, and **Occupation** features.
    
    Your answer goes here.
    
# Decision Tree Error Pruning

Shown in the figure below shows a decision tree for predicting heart disease. The descriptive features in this domain describe whether the patient suffers from chest pain (**Chest Pain**) and blood pressure (**Blood Pressure**). The binary target feature is **Heart Disease**. The table below the diagram lists a pruning set from this domain.

<!--

heart.dot file

digraph G {
   node [style=filled, color=lightblue]
   edge [style=solid, color=blue]


   A [label = "Chest Pain [true]"]
   B [label = "Blood Pressure [false]"]
   C [shape=box, label = "true"]
   D [shape=box, label = "true"]
   E [shape=box, label = "false"]

   A -> B [taillabel="false", labeldistance=5.5]
   A -> C [taillabel="true", labeldistance=2.5]
   B -> D [taillabel="high", labeldistance=3.5, labelangle=-30]
   B -> E [taillabel="low", labeldistance=3.5, labelangle=30]

}

Command to produce heart.png file

> dot -Tpng heart.dot -o heart.png
-->



![](./heart.png)





```{r}
#| echo: false
c1 <- c(1,2,3,4,5)
c2 <- c('false', 'true', 'false', 'true', 'false')
c3 <- c('high', 'low', 'low', 'high', 'high')
c4 <- c('false', 'true', 'false', 'true', 'false')
df11 <- data.frame(c1,c2,c3,c4)
# print(df11)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df11, caption = 'Pruning set.', col.names = c('ID', 'Chest Pain', 'Blood Pressure', 'Heart Disease'), align = "lrrr")
```



Using the pruning set, apply reduced error pruning to the decision tree. Assume that the algorithm is applied in a bottom-up, left-to-right fashion. For each iteration of the algorithm, indicate the subtrees considered as pruning candidates, explain why the algorithm chooses to prune or leave these subtrees in the tree, and illustrate the tree that results from each iteration.


Your answer goes here.



# Random Forest

The Table below lists a dataset containing the details of five participants in a heart disease study. The target feature *Risk* describes their risk of heart disease. Each patient is described in terms of four binary descriptive features:

- **Exercise** - how regularly do they exercise
- **Smoker** - do they smoke
- **Obese** - are they overweight
- **Family** - did any of their parents or siblings suffer from heart disease

```{r}
#| echo: false
ID <- c(1,2,3,4,5)
Exercise <- c('daily', 'weekly', 'daily', 'rarely', 'rarely')
Smoker <- c('false', 'true', 'false', 'true', 'true')
Obese <- c('false', 'false', 'false', 'true', 'true')
Family <- c('yes', 'yes', 'no', 'yes', 'no')
Risk <- c('low', 'high', 'low', 'high', 'high')
df3 <- data.frame(ID, Exercise, Smoker, Obese, Family, Risk)
# print(df3)
```
```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df3, caption = 'Heart disease study dataset.', align = "lrrrrr")
```


```{r}
#| echo: false
#| # Bootstrap Sample A
ID <- c(1,2,2,5,5)
Exercise <- c('daily', 'weekly', 'weekly', 'rarely', 'rarely')
Family <- c('yes', 'yes', 'yes', 'yes', 'no')
Risk <- c('low', 'high', 'high', 'high', 'high')
df4 <- data.frame(ID, Exercise, Family, Risk)
# print(df4)
```


```{r}
#| echo: false
#| # Bootstrap Sample B
ID <- c(1,2,2,4,5)
Smoker <- c('false', 'true', 'true', 'true', 'true')
Obese <- c('false', 'false', 'false', 'true', 'true')
Risk <- c('low', 'high', 'high', 'high', 'high')
df5 <- data.frame(ID, Smoker, Obese, Risk)
# print(df5)
```

```{r}
#| echo: false
#| # Bootstrap Sample C
ID <- c(1,1,2,4,5)
Obese <- c('false', 'false', 'false', 'true', 'true')
Family <- c('yes', 'yes', 'yes', 'yes', 'no')
Risk <- c('low', 'low', 'high', 'high', 'high')
df6 <- data.frame(ID, Obese, Family, Risk)
# print(df6)
```

As part of the study researchers have decided to create a predictive model to screen participants based on their risk of heart disease. You have been asked to implement this screening model using a random forest. Table below list three bootstrap samples that have been generated from the above dataset.

```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df4, caption = 'Heart disease study dataset bootstrap sample A.', align = "lrrrr")
```

```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df5, caption = 'Heart disease study dataset bootstrap sample B.', align = "lrrrr")
```
    
```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df6, caption = 'Heart disease study dataset bootstrap sample C.', align = "lrrrr")
```
    
    
1. Using these bootstrap samples create the decision trees that will be in the random forest model (use entropy based information gain as the feature selection criterion).
     
    Your answer goes here.
    
    
1. Assuming the random forest model you have created uses majority voting, what prediction will it return for the following query:

    Exercise=rarely, Smoker=false, Obese=true, Family=yes
    
    Your answer goes here.