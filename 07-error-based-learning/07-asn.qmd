---
title: "Written Assignment 07"
author: "Owen Senowitz"
date: today
date-format: long
number-sections: true
number-depth: 3
fig-cap-location: margin
format:
  html:
    toc: true
    toc-location: right
    number-sections: true
    number-depth: 3
    html-math-method: katex
    embed-resources: true
    self-contained: true
# bibliography: dasc-6000.bib 
# csl: ieee-with-url.csl
# linkcolor: red
# urlcolor: blue
# link-citations: yes
# header-includes:
#   - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Assignment Goal {.unnumbered}

The overarching goal for this assignment is to assess your understanding of the error-based machine learning algorithms. 

# Instructions {.unnumbered}

There are three questions. Each question carries 33 points.

Please show all your work. Simply providing the final answer is treated as no response. If you do not use R or Python notebooks, it is fine. However, the document structure should be preserved if you choose to use Microsoft Word or something else.

Please submit your response either as a self-contained HTML or PDF document.

# Multivariate Linear Regression Model

A multivariate linear regression model has been built to predict the heating load in a residential building based on a set of descriptive features describing the characteristics of the building. Heating load is the amount of heat energy required to keep a building at a specified temperature, usually 65 degrees Fahrenheit, during the winter regardless of outside temperature. The descriptive features used are the overall surface area of the building, the height of the building, the area of the building’s roof, and the percentage of wall area in the building that is glazed. This kind of model would be useful to architects or engineers when designing a new building. The trained model is:

$$
\begin{split}
\text{Heating Load} &= \text{-26.030 + 0.0497} \cdot \text{Surface Area}  \\
& + \text{4.942} \times \text{Height - 0.090} \times \text{Roof Area} + \text{20.523} \times \text{Glazing Area}
\end{split}
$$

Use this model to make predictions for each of the query instances shown in the table below.

```{r}
#| echo: false
library(readr)
df1 <- readr::read_csv("./heating-load.csv", col_names = TRUE)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df1, caption = 'Data for a predicting heating load in residential buildings.', align = "lrrrr")
```


ID 1:

$15.51 = -26.030 + 0.0497 \cdot 784.0 + 4.942 \cdot 3.5 - 0.090 \cdot 220.5 + 20.523 \cdot 0.25$

ID 2:

$7.22 = -26.030 + 0.0497 \cdot 710.5 + 4.942 \cdot 3.0 - 0.090 \cdot 210.5 + 20.523 \cdot 0.10$

ID 3:

$33.75 = -26.030 + 0.0497 \cdot 563.5 + 4.942 \cdot 7.0 - 0.090 \cdot 122.5 + 20.523 \cdot 0.40$

ID 4:

$34.36 = -26.030 + 0.0497 \cdot 637.0 + 4.942 \cdot 6.0 - 0.090 \cdot147.0 + 20.523 \cdot 0.60$


# Another Multivariate Linear Regression Model

You are asked to build a model that predicts the amount of oxygen that an astronaut consumes when performing five minutes of intense physical work. The descriptive features for the model will be the age of the astronaut and their average heart rate throughout the work. The regression model is:

\begin{align*}
\text{Oxycon} &= \text{w[0] + w[1]} \cdot \text{Age + w[2]} \cdot \text{Heart Rate} \\ 
\end{align*}

The table below shows a historical dataset that has been collected for this task.


```{r}
#| echo: false
library(readr)
df2 <- readr::read_csv("./oxygen-consumption.csv", col_names = TRUE)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df2, caption = 'Dataset for a predicting the amount of oxygen that an astronaut consumes when performing five minutes of intense physical work.', align = "lrrrr")
```

(a) Assuming that the current weights in a multivariate linear regression model are w[0]= -59.50, w[1] = -0.15, and w[2] = 0.60, make a prediction for each training instance using this model.

    1: $17.15 =-59.50 + (-0.15 \cdot 41) + (0.60 \cdot 138)$

    2: $26.00 = -59.50 + (-0.15 \cdot 42) + (0.60 \cdot 153)$

    3: $25.55 = -59.50 + (-0.15 \cdot 37) + (0.60 \cdot 151)$

    4: $13.40 = -59.50 + (-0.15 \cdot 46) + (0.60 \cdot 133)$

    5: $8.99 = -59.50 + (-0.15 \cdot 48) + (0.60 \cdot 126)$

    6: $20.90 = -59.50 + (-0.15 \cdot 44) + (0.60 \cdot 145)$

    7: $28.85 = -59.50 + (-0.15 \cdot 43) + (0.60 \cdot 158)$

    8: $19.40 = -59.50 + (-0.15 \cdot 46) + (0.60 \cdot 143)$

    9: $17.75 = -59.50 + (-0.15 \cdot 37) + (0.60 \cdot 138)$

    10: $29.60 = -59.50 + (-0.15 \cdot 38) + (0.60 \cdot 158)$

    11: $19.85 = -59.50 + (-0.15 \cdot 43) + (0.60 \cdot 143)$

    12: $16.85 = -59.50 + (-0.15 \cdot 43) + (0.60 \cdot 138)$

(b) Calculate the sum of squared errors for the set of predictions generated in part (a).

    1: $434.31 = (37.99 - 17.15)^2$

    2: $455.40 = (47.34 - 26.00)^2$

    3: $354.57 = (44.38 - 25.55)^2$

    4: $218.57 = (28.17 - 13.40)^2$

    5: $330.15 = (27.07 - 8.99)^2$

    6: $287.30 = (37.85 - 20.90)^2$

    7: $251.86 = (44.72 - 28.85)^2$

    8: $289.68 = (36.42 - 19.40)^2$

    9: $181.17 = (31.21 - 17.75)^2$

    10: $637.56 = (54.85 - 29.60)^2$

    11: $399.60 = (39.84 - 19.85)^2$

    12: $195.44 = (30.83 - 16.85)^2$

(c) Assuming a learning rate of 0.000002, calculate the weights at the next iteration of the gradient descent algorithm.

    $\text{Gradient Descent Update Formula:} \quad w_j = w_j - \eta \cdot \frac{\partial J}{\partial w_j}$

    $\text{For }w_0 = -59.50 - (0.000002 \cdot \frac{\partial J}{\partial w_0})$

    $\text{For }w_1 = -0.11 - (0.000002 \cdot \frac{\partial J}{\partial w_1})$

    $\text{For }w_2 = 0.73 - (0.000002 \cdot \frac{\partial J}{\partial w_2})$

    $w_0 = -59.50 - (0.000002 \cdot -2.33) = -59.50000466$

    $w_1 = -0.11 - (0.000002 \cdot -99.93) = -0.11019985$

    $w_2 = 0.73 - (0.000002 \cdot 331.49) = 0.72933702$

(d) Calculate the sum of squared errors for a set of predictions generated using the new set of weights calculated in part (c).

    $\text{Prediction Formula:} \quad \hat{y}_i = w_0 + w_1 \cdot \text{Age} + w_2 \cdot \text{Heart Rate}$

    $\text{For each ID:}$

    $\hat{y}_1 = -59.50000466 + (-0.11019985 \cdot 41) + (0.72933702 \cdot 138) = 35.91$

    $\hat{y}_2 = -59.50000466 + (-0.11019985 \cdot 42) + (0.72933702 \cdot 153) = 46.67$

    $\hat{y}_3 = -59.50000466 + (-0.11019985 \cdot 37) + (0.72933702 \cdot 151) = 45.79$

    $\hat{y}_4 = -59.50000466 + (-0.11019985 \cdot 46) + (0.72933702 \cdot 133) = 31.71$

    $\hat{y}_5 = -59.50000466 + (-0.11019985 \cdot 48) + (0.72933702 \cdot 126) = 26.41$

    $\hat{y}_6 = -59.50000466 + (-0.11019985 \cdot 44) + (0.72933702 \cdot 145) = 40.65$

    $\hat{y}_7 = -59.50000466 + (-0.11019985 \cdot 43) + (0.72933702 \cdot 158) = 50.18$

    $\hat{y}_8 = -59.50000466 + (-0.11019985 \cdot 46) + (0.72933702 \cdot 143) = 38.96$

    $\hat{y}_9 = -59.50000466 + (-0.11019985 \cdot 37) + (0.72933702 \cdot 138) = 36.36$

    $\hat{y}_{10} = -59.50000466 + (-0.11019985 \cdot 38) + (0.72933702 \cdot 158) = 50.75$

    $\hat{y}_{11} = -59.50000466 + (-0.11019985 \cdot 43) + (0.72933702 \cdot 143) = 39.31$

    $\hat{y}_{12} = -59.50000466 + (-0.11019985 \cdot 43) + (0.72933702 \cdot 138) = 35.68$

    $\text{Squared Error Formula:} \quad \text{SSE}_i = (y_i - \hat{y}_i)^2$

    $\text{For each ID:}$

    $\text{SSE}_1 = (37.99 - 35.91)^2 = 4.33$

    $\text{SSE}_2 = (47.34 - 46.67)^2 = 0.45$

    $\text{SSE}_3 = (44.38 - 45.79)^2 = 1.99$

    $\text{SSE}_4 = (28.17 - 31.71)^2 = 12.60$

    $\text{SSE}_5 = (27.07 - 26.41)^2 = 0.423$

    $\text{SSE}_6 = (37.85 - 40.65)^2 = 7.81$

    $\text{SSE}_7 = (44.72 - 50.18)^2 = 29.86$

    $\text{SSE}_8 = (36.42 - 38.96)^2 = 6.49$

    $\text{SSE}_9 = (31.21 - 36.36)^2 = 26.56$

    $\text{SSE}_{10} = (54.85 - 50.75)^2 = 16.79$

    $\text{SSE}_{11} = (39.84 - 39.31)^2 = 0.28$

    $\text{SSE}_{12} = (30.83 - 35.68)^2 = 23.55$

    $\text{Total SSE:} \quad \text{SSE} = \sum_{i=1}^{n} \text{SSE}_i$

    $\text{Final SSE:} \quad \text{SSE} = 164.16$



# Logistic Regression Model

The effects that can occur when different drugs are taken together can be difficult for doctors to predict. Machine learning models can be built to help predict optimal dosages of drugs so as to achieve a medical practitioner’s goals.26 In the following figure, the image on the left shows a scatter plot of a dataset used to train a model to distinguish between dosages of two drugs that cause a dangerous interaction and those that cause a safe interaction. There are just two continuous features in this dataset, DOSE1 and DOSE2 (both normalized to the range $(-1,1)$ using range normalization), and two target levels, *dangerous* and *safe*. In the scatter plot, DOSE1 is shown on the horizontal axis, DOSE2 is shown on the vertical axis, and the shapes of the points represent the target level—crosses represent *dangerous* interactions and triangles represent *safe* interactions.

![A scatter plot of drug interactions.](./drug-interactions.png)

In the preceding figure, the image on the right shows a simple linear logistic regression model trained to perform this task. This model is:

P(TYPE = dangerous) =  *Logistic*(0.6168 + 2.7320 $\times$ DOSE1 + 2.4809 $\times$ DOSE2)


Plainly, this model is not performing well.


(a) Would the similarity-based, information-based, or probability-based predictive modeling approaches already covered in this book be likely to do a better job of learning this model than the simple linear regression model?

    The similarity-based, information-based, and probability-based approaches are likely to outperform the simple linear logistic regression model in this scenario. Methods like k-Nearest Neighbors (similarity-based) can handle the non-linear decision boundary needed to classify the data effectively. Information-based approaches, such as decision trees, can create complex splits in the feature space, making them well-suited for distinguishing between dangerous and safe interactions. Additionally, probability-based methods, like Naive Bayes or more advanced probabilistic models, may better capture the underlying patterns if the feature relationships align with their assumptions. These models are generally more flexible and adaptable to non-linear patterns compared to simple linear models.
    
    
(b) A simple approach to adapting a logistic regression model to learn this type of decision boundary is to introduce a set of basis functions that will allow a non-linear decision boundary to be learned. In this case, a set of basis functions that generate a cubic decision boundary will work well. An appropriate set of basis functions is as follows:

$$
\begin{aligned}
& \phi_0(\langle\text { DOSE1 }, \text { DOSE2 } \rangle)=1 \quad \phi_1(\langle\text { DOSE1 }, \text {DOSE2 } \rangle)=\text { DOSE1 }  \\
& \phi_2(\langle\text { DOSE1 }, \text { DOSE2 } \rangle)=\text { DOSE2 } \quad \phi_3(\langle\text { DOSE1 }, \text { DOSE2 } \rangle) = \text { DOSE1}^2 \\
& \phi_4(\langle\text { DOSE1 }, \text { DOSE2 } \rangle)=\operatorname{DOSE2}^2 \quad \phi_5(\langle\text { DOSE1 }, \text { DOSE2 } \rangle)=\operatorname{DOSE1}^3 \\
& \phi_6(\langle\text { DOSE1 }, \text { DOSE2 } \rangle)=\operatorname{DOSE2}^3 \quad \phi_7(\langle\text { DOSE1 }, \text { DOSE2 } \rangle)=\text { DOSE1 } \times \text { DOSE2 } \\
&
\end{aligned}
$$


Training a logistic regression model using this set of basis functions leads to the following model:

$$
\begin{aligned}
& P(\text { TYPE }=\text { dangerous })= \\
& \operatorname{Logistic}\left(-0.848 \times \phi_0(\langle\text { DOSE1, DOSE2} \rangle)+1.545 \times \phi_1(\langle\text { DOSE1, DOSE2} \rangle)\right. \\
& -1.942 \times \phi_2(\langle\text { DOSE1, DOSE2} \rangle)+1.973 \times \phi_3(\langle\text { DOSE1}, \text { DOSE2 }\rangle) \\
& +2.495 \times \phi_4(\langle\text { DOSE1}, \text { DOSE2} \rangle)+0.104 \times \phi_5(\langle\text { DOSE1, DOSE2} \rangle) \\
& \left.+0.095 \times \phi_6(\langle\text { DOSE1}, \text { DOSE2} \rangle)+3.009 \times \phi_7(\langle\text { DOSE1, DOSE} 2\rangle)\right) \\
&
\end{aligned}
$$

Use this model to make predictions for the following query instances:

```{r}
#| echo: false
library(readr)
df3 <- readr::read_csv("./dose.csv", col_names = TRUE)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df3, caption = 'Prediction queries.', align = "lrrrr")
```


    
$\text{Logistic Regression Formula:} \quad P(\text{TYPE} = \text{dangerous}) = \frac{1}{1 + e^{-Z}}$

$\text{Z Formula:} \quad Z = -0.848 \cdot \phi_0 + 1.545 \cdot \phi_1 - 1.942 \cdot \phi_2 + 1.973 \cdot \phi_3 + 2.495 \cdot \phi_4 + 0.104 \cdot \phi_5 + 0.095 \cdot \phi_6 + 3.009 \cdot \phi_7$

$\text{Basis Functions:}$
$\phi_0 = 1$
$\phi_1 = \text{DOSE1}$
$\phi_2 = \text{DOSE2}$
$\phi_3 = \text{DOSE1}^2$
$\phi_4 = \text{DOSE2}^2$
$\phi_5 = \text{DOSE1}^3$
$\phi_6 = \text{DOSE2}^3$
$\phi_7 = \text{DOSE1} \cdot \text{DOSE2}$

$\text{For ID 1:}$
$Z = -0.848 \cdot 1 + 1.545 \cdot 0.50 - 1.942 \cdot 0.75 + 1.973 \cdot 0.2500 + 2.495 \cdot 0.5625 + 0.104 \cdot 0.125000 + 0.095 \cdot 0.421875 + 3.009 \cdot 0.3750 = 1.546141$
$P(\text{TYPE} = \text{dangerous}) = \frac{1}{1 + e^{-1.546141}} = 0.824356$

$\text{For ID 2:}$
$Z = -0.848 \cdot 1 + 1.545 \cdot 0.10 - 1.942 \cdot 0.75 + 1.973 \cdot 0.0100 + 2.495 \cdot 0.5625 + 0.104 \cdot 0.001000 + 0.095 \cdot 0.421875 + 3.009 \cdot 0.0750 = -0.460975$
$P(\text{TYPE} = \text{dangerous}) = \frac{1}{1 + e^{0.460975}} = 0.386754$

$\text{For ID 3:}$
$Z = -0.848 \cdot 1 + 1.545 \cdot -0.47 - 1.942 \cdot -0.39 + 1.973 \cdot 0.2209 + 2.495 \cdot 0.1521 + 0.104 \cdot -0.103823 + 0.095 \cdot -0.059319 + 3.009 \cdot 0.1833 = 0.533672$
$P(\text{TYPE} = \text{dangerous}) = \frac{1}{1 + e^{-0.533672}} = 0.630339$

$\text{For ID 4:}$
$Z = -0.848 \cdot 1 + 1.545 \cdot -0.47 - 1.942 \cdot 0.18 + 1.973 \cdot 0.2209 + 2.495 \cdot 0.0324 + 0.104 \cdot -0.103823 + 0.095 \cdot 0.005832 + 3.009 \cdot -0.0846 = -1.671841$
$P(\text{TYPE} = \text{dangerous}) = \frac{1}{1 + e^{1.671841}} = 0.158179$
